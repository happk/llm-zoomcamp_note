{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658f909-e679-41e9-9c4e-e0241c719049",
   "metadata": {},
   "source": [
    "If you're not running in Saturn Cloud, you need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "```\n",
    "pip install -U transformers accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff0f94",
   "metadata": {},
   "source": [
    "## 第一章的內容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fab2a-a50c-42bd-a106-c83a9d2828ea",
   "metadata": {},
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py -O minsearch.py  # powershell的指令寫法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39574e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x26db303b1c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f087272-b44d-4738-9ea2-175ec63a058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=3\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ab881-499a-4675-83c4-2013ea1377b9",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a77e6-936b-448e-a04b-bad1001f5bb0",
   "metadata": {},
   "source": [
    "## 嘗試新模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21aa255e-c971-44ca-9826-a721df3ad063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先設置模型下載位置\n",
    "import os\n",
    "os.environ['HF_HOME'] = '../../models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89985a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975bb703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9348,   25,  482,    3,    9, 3714,   81,  140,    6,   82,  564,   19,\n",
       "            3,   15, 2234,   58,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c1ac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  6964,    19,     3,     9,  1021,  4940,   113,  1342,    28,\n",
      "           112,  1362,    16,     3,     9,   422,  1511,    16,     8,  2214,\n",
      "            13, 16715,     5,   555,   239,     6,  6964,  7864,     3,     9,\n",
      "          3202,  2650,  8077,    11,    79,  1590,    16,   333,     5,  6964,\n",
      "            19,   182,  1095,    11,  2764,    13,   112,   126,  1675,     5,\n",
      "          8077,    19,   182,  1095,    11,  2764,    13,  6964,     5,  6964,\n",
      "            11,  8077,   129,  4464,    11,    43,     3,     9,  1871,  3202,\n",
      "             5,  6964,    19,   182,  1095,    11,  2764,    13,   112,   126,\n",
      "          1675,     5,  8077,    19,   182,  1095,    11,  2764,    13,  6964,\n",
      "             5,  8077,    11,  6964,    33,   182,  1095,    11,  2764,    13]],\n",
      "       device='cuda:0')\n",
      "<pad>Eric is a young boy who lives with his parents in a small town in the middle of nowhere. One day, Eric meets a girl named Sarah and they fall in love. Eric is very happy and proud of his new relationship. Sarah is very happy and proud of Eric. Eric and Sarah get married and have a baby girl. Eric is very happy and proud of his new relationship. Sarah is very happy and proud of Eric. Sarah and Eric are very happy and proud of\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids,max_length=100)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf5769",
   "metadata": {},
   "source": [
    "## 結合在RAG中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "942fdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids,max_length=100)\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    return result\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c7d5b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> CONTEXT: section: Module 2: Workflow Orchestration</s>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('I missed the class. What should I do?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368476ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53400ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看起來在長文的情況下，會報錯，可能是因為flan-t5-large模型的問題，先嘗試其他模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda5176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
